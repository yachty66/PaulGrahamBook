<b>Heresy</b><br><font face="verdana" size="2">
 April 2022
 <br/>
 <br/>
 One of the most surprising things I've witnessed in my lifetime is
the rebirth of the concept of heresy.
 <br/>
 <br/>
 In his excellent biography of Newton, Richard Westfall writes about the
moment when he was elected a fellow of Trinity College:
 <blockquote>
  Supported comfortably, Newton was free to devote himself wholly
  to whatever he chose. To remain on, he had only to avoid the three
  unforgivable sins: crime, heresy, and marriage.
  <font color="#dddddd">
   [
   <a href="#f1n">
    <font color="#dddddd">
     1
    </font>
   </a>
   ]
  </font>
 </blockquote>
 The first time I read that, in the 1990s, it sounded amusingly
medieval. How strange, to have to avoid committing heresy. But when
I reread it 20 years later it sounded like a description of
contemporary employment.
 <br/>
 <br/>
 There are an ever-increasing number of opinions you can be fired
for. Those doing the firing don't use the word "heresy" to describe
them, but structurally they're equivalent. Structurally there are
two distinctive things about heresy: (1) that it takes priority
over the question of truth or falsity, and (2) that it outweighs
everything else the speaker has done.
 <br/>
 <br/>
 For example, when someone calls a statement "x-ist," they're also
implicitly saying that this is the end of the discussion. They do
not, having said this, go on to consider whether the statement is
true or not. Using such labels is the conversational equivalent of
signalling an exception. That's one of the reasons they're used:
to end a discussion.
 <br/>
 <br/>
 If you find yourself talking to someone who uses these labels a
lot, it might be worthwhile to ask them explicitly if they believe
any babies are being thrown out with the bathwater. Can a statement
be x-ist, for whatever value of x, and also true? If the answer is
yes, then they're admitting to banning the truth. That's obvious
enough that I'd guess most would answer no. But if they answer no,
it's easy to show that they're mistaken, and that in practice such
labels are applied to statements regardless of their truth or
falsity.
 <br/>
 <br/>
 The clearest evidence of this is that whether a statement is
considered x-ist often depends on who said it. Truth doesn't work
that way. The same statement can't be true when one person says it,
but x-ist, and therefore false, when another person does.
 <font color="#dddddd">
  [
  <a href="#f2n">
   <font color="#dddddd">
    2
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 The other distinctive thing about heresies, compared to ordinary
opinions, is that the public expression of them outweighs everything
else the speaker has done. In ordinary matters, like knowledge of
history, or taste in music, you're judged by the average of your
opinions. A heresy is qualitatively different. It's like dropping
a chunk of uranium onto the scale.
 <br/>
 <br/>
 Back in the day (and still, in some places) the punishment for
heresy was death. You could have led a life of exemplary goodness,
but if you publicly doubted, say, the divinity of Christ, you were
going to burn. Nowadays, in civilized countries, heretics only get
fired in the metaphorical sense, by losing their jobs. But the
structure of the situation is the same: the heresy
outweighs everything else. You could have spent the last ten years
saving children's lives, but if you express certain opinions, you're
automatically fired.
 <br/>
 <br/>
 It's much the same as if you committed a crime. No matter how
virtuously you've lived, if you commit a crime, you must still
suffer the penalty of the law. Having lived a previously blameless
life might mitigate the punishment, but it doesn't affect whether
you're guilty or not.
 <br/>
 <br/>
 A heresy is an opinion whose expression is treated like a crime —
one that makes some people feel not merely that you're mistaken,
but that you should be punished. Indeed, their desire to see you
punished is often stronger than it would be if you'd committed an
actual crime. There are many on the far left who believe
strongly in the reintegration of felons (as I do myself), and yet
seem to feel that anyone guilty of certain heresies should never
work again.
 <br/>
 <br/>
 There are always some heresies — some opinions you'd be punished
for expressing. But there are a lot more now than there were a few
decades ago, and even those who are happy about this would have to
agree that it's so.
 <br/>
 <br/>
 Why? Why has this antiquated-sounding religious concept come back
in a secular form? And why now?
 <br/>
 <br/>
 You need two ingredients for a wave of intolerance: intolerant
people, and an ideology to guide them. The intolerant people are
always there. They exist in every sufficiently large society. That's
why waves of intolerance can arise so suddenly; all they need is
something to set them off.
 <br/>
 <br/>
 I've already written an
 <a href="conformism.html">
  <u>
   essay
  </u>
 </a>
 describing the aggressively
conventional-minded. The short version is that people can be
classified in two dimensions according to (1) how independent- or
conventional-minded they are, and (2) how aggressive they are about
it. The aggressively conventional-minded are the enforcers of
orthodoxy.
 <br/>
 <br/>
 Normally they're only locally visible. They're the grumpy, censorious
people in a group — the ones who are always first to complain when
something violates the current rules of propriety. But occasionally,
like a vector field whose elements become aligned, a large number
of aggressively conventional-minded people unite behind some ideology
all at once. Then they become much more of a problem, because a mob
dynamic takes over, where the enthusiasm of each participant is
increased by the enthusiasm of the others.
 <br/>
 <br/>
 The most notorious 20th century case may have been the Cultural
Revolution. Though initiated by Mao to undermine his rivals, the
Cultural Revolution was otherwise mostly a grass-roots phenomenon.
Mao said in essence: There are heretics among us. Seek them out and
punish them. And that's all the aggressively conventional-minded
ever need to hear. They went at it with the delight of dogs chasing
squirrels.
 <br/>
 <br/>
 To unite the conventional-minded, an ideology must have many of the
features of a religion. In particular it must have strict and
arbitrary rules that adherents can demonstrate their
 <a href="https://www.youtube.com/watch?v=qaHLd8de6nM">
  <u>
   purity
  </u>
 </a>
 by obeying, and its adherents must believe that anyone who obeys these
rules is ipso facto morally superior to anyone who doesn't.
 <font color="#dddddd">
  [
  <a href="#f3n">
   <font color="#dddddd">
    3
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 In the late 1980s a new ideology of this type appeared in US
universities. It had a very strong component of moral purity, and
the aggressively conventional-minded seized upon it with their usual
eagerness — all the more because the relaxation of social norms
in the preceding decades meant there had been less and less to
forbid. The resulting wave of intolerance has been eerily similar
in form to the Cultural Revolution, though fortunately much smaller
in magnitude.
 <font color="#dddddd">
  [
  <a href="#f4n">
   <font color="#dddddd">
    4
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 I've deliberately avoided mentioning any specific heresies here.
Partly because one of the universal tactics of heretic hunters, now
as in the past, is to accuse those who disapprove of the way in
which they suppress ideas of being heretics themselves. Indeed,
this tactic is so consistent that you could use it as a way of
detecting witch hunts in any era.
 <br/>
 <br/>
 And that's the second reason I've avoided mentioning any specific
heresies. I want this essay to work in the future, not just now.
And unfortunately it probably will. The aggressively conventional-minded
will always be among us, looking for things to forbid. All they
need is an ideology to tell them what. And it's unlikely the current
one will be the last.
 <br/>
 <br/>
 There are aggressively conventional-minded people on both the right
and the left. The reason the current wave of intolerance comes from
the left is simply because the new unifying ideology happened to
come from the left. The next one might come from the right. Imagine
what that would be like.
 <br/>
 <br/>
 Fortunately in western countries the suppression of heresies is
nothing like as bad as it used to be. Though the window of opinions
you can express publicly has narrowed in the last decade, it's still
much wider than it was a few hundred years ago. The problem is the
derivative. Up till about 1985 the window had been growing ever
wider. Anyone looking into the future in 1985 would have expected
freedom of expression to continue to increase. Instead it has
decreased.
 <font color="#dddddd">
  [
  <a href="#f5n">
   <font color="#dddddd">
    5
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 The situation is similar to what's happened with infectious diseases
like measles. Anyone looking into the future in 2010 would have
expected the number of measles cases in the US to continue to
decrease. Instead, thanks to anti-vaxxers, it has increased. The
absolute number is still not that high. The problem is the derivative.
 <font color="#dddddd">
  [
  <a href="#f6n">
   <font color="#dddddd">
    6
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 In both cases it's hard to know how much to worry. Is it really
dangerous to society as a whole if a handful of extremists refuse
to get their kids vaccinated, or shout down speakers at universities?
The point to start worrying is presumably when their efforts start
to spill over into everyone else's lives. And in both cases that
does seem to be happening.
 <br/>
 <br/>
 So it's probably worth spending some amount of effort on pushing
back to keep open the window of free expression. My hope is that
this essay will help form social antibodies not just against current
efforts to suppress ideas, but against the concept of heresy in
general. That's the real prize. How do you disable the concept of
heresy? Since the Enlightenment, western societies have discovered
many techniques for doing that, but there are surely more to be
discovered.
 <br/>
 <br/>
 Overall I'm optimistic. Though the trend in freedom of expression
has been bad over the last decade, it's been good over the longer
term. And there are signs that the current wave of intolerance is
peaking. Independent-minded people I talk to seem more confident
than they did a few years ago. On the other side, even some of the
 <a href="https://www.nytimes.com/2022/03/18/opinion/cancel-culture-free-speech-poll.html">
  <u>
   leaders
  </u>
 </a>
 are starting to wonder if things have 
gone too far. And popular culture among the young has already moved on. 
All we have
to do is keep pushing back, and the wave collapses. And then we'll
be net ahead, because as well as having defeated this wave, we'll
also have developed new tactics for resisting the next one.
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 [
 ] 
Or more accurately, biographies of Newton, since Westfall wrote
two: a long version called
 , and a shorter one called
 . Both are great. The short version moves
faster, but the long one is full of interesting and often very funny
details. This passage is the same in both.
 [
 ]
Another more subtle but equally damning bit of evidence is
that claims of x-ism are never qualified. You never hear anyone say
that a statement is "probably x-ist" or "almost certainly y-ist."
If claims of x-ism were actually claims about truth, you'd expect
to see "probably" in front of "x-ist" as often as you see it in
front of "fallacious."
 [
 ] 
The rules must be strict, but they need not be demanding. So
the most effective type of rules are those about superficial matters,
like doctrinal minutiae, or the precise words adherents must use.
Such rules can be made extremely complicated, and yet don't repel
potential converts by requiring significant sacrifice.
 The superficial demands of orthodoxy make it an inexpensive substitute
for virtue. And that in turn is one of the reasons orthodoxy is so
attractive to bad people. You could be a horrible person, and yet
as long as you're orthodox, you're better than everyone who isn't.
 [
 ] 
Arguably there were two. The first had died down somewhat by
2000, but was followed by a second in the 2010s, probably caused
by social media.
 [
 ] 
Fortunately most of those trying to suppress ideas today still
respect Enlightenment principles enough to pay lip service to them.
They know they're not supposed to ban ideas per se, so they have
to recast the ideas as causing "harm," which sounds like something
that can be banned. The more extreme try to claim speech itself is
violence, or even that silence is. But strange as it may sound,
such gymnastics are a good sign. We'll know we're really in trouble
when they stop bothering to invent pretenses for banning ideas —
when, like the medieval church, they say "Damn right we're banning
ideas, and in fact here's a list of them."
 [
 ] 
People only have the luxury of ignoring the medical consensus
about vaccines because vaccines have worked so well. If we didn't
have any vaccines at all, the mortality rate would be so high that
most current anti-vaxxers would be begging for them. And the situation
with freedom of expression is similar. It's only because they live
in a world created by the Enlightenment that kids from the suburbs
can play at banning ideas.
</font><br><br><b>Putting Ideas into Words</b><br><font face="verdana" size="2">
 February 2022
 <br/>
 <br/>
 Writing about something, even something you know well, usually shows
you that you didn't know it as well as you thought. Putting ideas
into words is a severe test. The first words you choose are usually
wrong; you have to rewrite sentences over and over
 <!-- if you want -->
 to
get them exactly right. And your ideas won't just be imprecise, but
incomplete too. Half the ideas that end up in an essay will be ones
you thought of while you were writing it. Indeed, that's why I write
them.
 <br/>
 <br/>
 Once you publish something, the convention is that whatever you
wrote was what you thought before you wrote it. These were your
ideas, and now you've expressed them. But you know this isn't true.
You know that putting your ideas into words changed them. And not
just the ideas you published. Presumably there were others that
turned out to be too broken to fix, and those you discarded instead.
 <br/>
 <br/>
 It's not just having to commit your ideas to specific words that
makes writing so exacting. The real test is reading what you've
written. You have to pretend to be a neutral reader who knows nothing
of what's in your head, only what you wrote. When he reads what you
wrote, does it seem correct? Does it seem complete? If you make an
effort, you can read your writing as if you were a complete stranger,
and when you do the news is usually bad. It takes me many cycles
before I can get an essay past the stranger. But the stranger is
rational, so you always can, if you ask him what he needs. If he's
not satisfied because you failed to mention x or didn't qualify
some sentence sufficiently, then you mention x or add more
qualifications. Happy now? It may cost you some nice sentences, but
you have to resign yourself to that. You just have to make them as
good as you can and still satisfy the stranger.
 <br/>
 <br/>
 This much, I assume, won't be that controversial. I think it will
accord with the experience of anyone who has tried to write about
anything nontrivial. There may exist people whose thoughts are so
perfectly formed that they just flow straight into words. But I've
never known anyone who could do this, and if I met someone who said
they could, it would seem evidence of their limitations rather than
their ability. Indeed, this is a trope in movies: the guy who claims
to have a plan for doing some difficult thing, and who when questioned
further, taps his head and says "It's all up here." Everyone watching
the movie knows what that means. At best the plan is vague and
incomplete. Very likely there's some undiscovered flaw that invalidates
it completely. At best it's a plan for a plan.
 <br/>
 <br/>
 In precisely defined domains it's possible to form complete ideas
in your head. People can play chess in their heads, for example.
And mathematicians can do some amount of math in their heads, though
they don't seem to feel sure of a proof over a certain length till
they write it down. But this only seems possible with ideas you can
express in a formal language.
 <font color="#dddddd">
  [
  <a href="#f1n">
   <font color="#dddddd">
    1
   </font>
  </a>
  ]
 </font>
 Arguably what such people are
doing is putting ideas into words in their heads. I can to some
extent write essays in my head. I'll sometimes think of a paragraph
while walking or lying in bed that survives nearly unchanged in the
final version. But really I'm writing when I do this. I'm doing the
mental part of writing; my fingers just aren't moving as I do it.
 <font color="#dddddd">
  [
  <a href="#f2n">
   <font color="#dddddd">
    2
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 You can know a great deal about something without writing about it.
Can you ever know so much that you wouldn't learn more from trying
to explain what you know? I don't think so. I've written about at
least two subjects I know well — Lisp hacking and startups
— and in both cases I learned a lot from writing about them.
In both cases there were things I didn't consciously realize till
I had to explain them. And I don't think my experience was anomalous.
A great deal of knowledge is unconscious, and experts have if
anything a higher proportion of unconscious knowledge than beginners.
 <br/>
 <br/>
 I'm not saying that writing is the best way to explore all ideas.
If you have ideas about architecture, presumably the best way to
explore them is to build actual buildings. What I'm saying is that
however much you learn from exploring ideas in other ways, you'll
still learn new things from writing about them.
 <br/>
 <br/>
 Putting ideas into words doesn't have to mean writing, of course.
You can also do it the old way, by talking. But in my experience,
writing is the stricter test. You have to commit to a single, optimal
sequence of words. Less can go unsaid when you don't have tone of
voice to carry meaning. And you can focus in a way that would seem
excessive in conversation. I'll often spend 2 weeks on an essay and
reread drafts 50 times. If you did that in conversation
it would seem evidence of some kind of
mental disorder. 
If you're lazy,
of course, writing and talking are equally useless. But if you want
to push yourself to get things right, writing is the steeper hill.
 <font color="#dddddd">
  [
  <a href="#f3n">
   <font color="#dddddd">
    3
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 The reason I've spent so long establishing this rather obvious point
is that it leads to another that many people will find shocking.
If writing down your ideas always makes them more precise and more
complete, then no one who hasn't written about a topic has fully
formed ideas about it. And someone who never writes has no fully
formed ideas about anything nontrivial.
 <br/>
 <br/>
 It feels to them as if they do, especially if they're not in the
habit of critically examining their own thinking. Ideas can feel
complete. It's only when you try to put them into words that you
discover they're not. So if you never subject your ideas to that
test, you'll not only never have fully formed ideas, but also never
realize it.
 <br/>
 <br/>
 Putting ideas into words is certainly no guarantee that they'll be
right. Far from it. But though it's not a sufficient condition, it
is a necessary one.
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 [
 ] Machinery and
circuits are formal languages.
 [
 ] I thought of this
sentence as I was walking down the street in Palo Alto.
 [
 ] There are two
senses of talking to someone: a strict sense in which the conversation
is verbal, and a more general sense in which it can take any form,
including writing. In the limit case (e.g. Seneca's letters),
conversation in the latter sense becomes essay writing.
 It can be very useful to talk (in either sense) with other people
as you're writing something. But a verbal conversation will never
be more exacting than when you're talking about something you're
writing.
</font><br><br><b>Is There Such a Thing as Good Taste?</b><br><font face="verdana" size="2">
 November 2021
 <br/>
 <br/>
 <i>
  (This essay is derived from a talk at the Cambridge Union.)
 </i>
 <br/>
 <br/>
 When I was a kid, I'd have said there wasn't. My father told me so.
Some people like some things, and other people like other things,
and who's to say who's right?
 <br/>
 <br/>
 It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that's what I'm going to give you here: a proof by
reductio ad absurdum. If we start from the premise that there's no
such thing as good taste, we end up with conclusions that are
obviously false, and therefore the premise must be wrong.
 <br/>
 <br/>
 We'd better start by saying what good taste is. There's a narrow
sense in which it refers to aesthetic judgements and a broader one
in which it refers to preferences of any kind. The strongest proof
would be to show that taste exists in the narrowest sense, so I'm
going to talk about taste in art. You have better taste than me if
the art you like is better than the art I like.
 <br/>
 <br/>
 If there's no such thing as good taste, then there's no such thing
as
 <a href="goodart.html">
  <u>
   good art
  </u>
 </a>
 . Because if there is such a
thing as good art, it's
easy to tell which of two people has better taste. Show them a lot
of works by artists they've never seen before and ask them to
choose the best, and whoever chooses the better art has better
taste.
 <br/>
 <br/>
 So if you want to discard the concept of good taste, you also have
to discard the concept of good art. And that means you have to
discard the possibility of people being good at making it. Which
means there's no way for artists to be good at their jobs. And not
just visual artists, but anyone who is in any sense an artist. You
can't have good actors, or novelists, or composers, or dancers
either. You can have popular novelists, but not good ones.
 <br/>
 <br/>
 We don't realize how far we'd have to go if we discarded the concept
of good taste, because we don't even debate the most obvious cases.
But it doesn't just mean we can't say which of two famous painters
is better. It means we can't say that any painter is better than a
randomly chosen eight year old.
 <br/>
 <br/>
 That was how I realized my father was wrong. I started studying
painting. And it was just like other kinds of work I'd done: you
could do it well, or badly, and if you tried hard, you could get
better at it. And it was obvious that Leonardo and Bellini were
much better at it than me. That gap between us was not imaginary.
They were so good. And if they could be good, then art could be
good, and there was such a thing as good taste after all.
 <br/>
 <br/>
 Now that I've explained how to show there is such a thing as good
taste, I should also explain why people think there isn't. There
are two reasons. One is that there's always so much disagreement
about taste. Most people's response to art is a tangle of unexamined
impulses. Is the artist famous? Is the subject attractive? Is this
the sort of art they're supposed to like? Is it hanging in a famous
museum, or reproduced in a big, expensive book? In practice most
people's response to art is dominated by such extraneous factors.
 <br/>
 <br/>
 And the people who do claim to have good taste are so often mistaken.
The paintings admired by the so-called experts in one generation
are often so different from those admired a few generations later.
It's easy to conclude there's nothing real there at all. It's only
when you isolate this force, for example by trying to paint and
comparing your work to Bellini's, that you can see that it does in
fact exist.
 <br/>
 <br/>
 The other reason people doubt that art can be good is that there
doesn't seem to be any room in the art for this goodness. The
argument goes like this. Imagine several people looking at a work
of art and judging how good it is. If being good art really is a
property of objects, it should be in the object somehow. But it
doesn't seem to be; it seems to be something happening in the heads
of each of the observers. And if they disagree, how do you choose
between them?
 <br/>
 <br/>
 The solution to this puzzle is to realize that the purpose of art
is to work on its human audience, and humans have a lot in common.
And to the extent the things an object acts upon respond in the
same way, that's arguably what it means for the object to have the
corresponding property. If everything a particle interacts with
behaves as if the particle had a mass of
 <i>
  m
 </i>
 , then it has a mass of
 <i>
  m
 </i>
 . So the distinction between "objective" and "subjective" is not
binary, but a matter of degree, depending on how much the subjects
have in common. Particles interacting with one another are at one
pole, but people interacting with art are not all the way at the
other; their reactions aren't
 <i>
  random
 </i>
 .
 <br/>
 <br/>
 Because people's responses to art aren't random, art can be designed
to operate on people, and be good or bad depending on how effectively
it does so. Much as a vaccine can be. If someone were talking about
the ability of a vaccine to confer immunity, it would seem very
frivolous to object that conferring immunity wasn't really a property
of vaccines, because acquiring immunity is something that happens
in the immune system of each individual person. Sure, people's
immune systems vary, and a vaccine that worked on one might not
work on another, but that doesn't make it meaningless to talk about
the effectiveness of a vaccine.
 <br/>
 <br/>
 The situation with art is messier, of course. You can't measure
effectiveness by simply taking a vote, as you do with vaccines.
You have to imagine the responses of subjects with a deep knowledge
of art, and enough clarity of mind to be able to ignore extraneous
influences like the fame of the artist. And even then you'd still
see some disagreement. People do vary, and judging art is hard,
especially recent art. There is definitely not a total order either
of works or of people's ability to judge them. But there is equally
definitely a partial order of both. So while it's not possible to
have perfect taste, it is possible to have good taste.
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <font color="888888">
  to the Cambridge Union for inviting me, and to Trevor
Blackwell, Jessica Livingston, and Robert Morris for reading drafts
of this.
 </font>
</font><br><br><b>Beyond Smart</b><br><font face="verdana" size="2">
 October 2021
 <br/>
 <br/>
 If you asked people what was special about Einstein, most would say
that he was really smart. Even the ones who tried to give you a
more sophisticated-sounding answer would probably think this first.
Till a few years ago I would have given the same answer myself. But
that wasn't what was special about Einstein. What was special about
him was that he had important new ideas. Being very smart was a
necessary precondition for having those ideas, but the two are not
identical.
 <br/>
 <br/>
 It may seem a hair-splitting distinction to point out that intelligence
and its consequences are not identical, but it isn't. There's a big
gap between them. Anyone who's spent time around universities and
research labs knows how big. There are a lot of genuinely smart
people who don't achieve very much.
 <br/>
 <br/>
 I grew up thinking that being smart was the thing most to be desired.
Perhaps you did too. But I bet it's not what you really want. Imagine
you had a choice between being really smart but discovering nothing
new, and being less smart but discovering lots of new ideas. Surely
you'd take the latter. I would. The choice makes me uncomfortable,
but when you see the two options laid out explicitly like that,
it's obvious which is better.
 <br/>
 <br/>
 The reason the choice makes me uncomfortable is that being smart
still feels like the thing that matters, even though I know
intellectually that it isn't. I spent so many years thinking it
was. The circumstances of childhood are a perfect storm for fostering
this illusion. Intelligence is much easier to measure than the value
of new ideas, and you're constantly being judged by it. Whereas
even the kids who will ultimately discover new things aren't usually
discovering them yet. For kids that way inclined, intelligence is
the only game in town.
 <br/>
 <br/>
 There are more subtle reasons too, which persist long into adulthood.
Intelligence wins in conversation, and thus becomes the basis of
the dominance hierarchy.
 <font color="#dddddd">
  [
  <a href="#f1n">
   <font color="#dddddd">
    1
   </font>
  </a>
  ]
 </font>
 Plus having new ideas is such a new
thing historically, and even now done by so few people, that society
hasn't yet assimilated the fact that this is the actual destination,
and intelligence merely a means to an end.
 <font color="#dddddd">
  [
  <a href="#f2n">
   <font color="#dddddd">
    2
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 Why do so many smart people fail to discover anything new? Viewed
from that direction, the question seems a rather depressing one.
But there's another way to look at it that's not just more optimistic,
but more interesting as well. Clearly intelligence is not the only
ingredient in having new ideas. What are the other ingredients?
Are they things we could cultivate?
 <br/>
 <br/>
 Because the trouble with intelligence, they say, is that it's mostly
inborn. The evidence for this seems fairly convincing, especially
considering that most of us don't want it to be true, and the
evidence thus has to face a stiff headwind. But I'm not going
to get into that question here, because it's the other ingredients
in new ideas that I care about, and it's clear that many of them
can be cultivated.
 <br/>
 <br/>
 That means the truth is excitingly different from the story I got
as a kid. If intelligence is what matters, and also mostly inborn,
the natural consequence is a sort of
 <i>
  Brave New World
 </i>
 fatalism. The
best you can do is figure out what sort of work you have an "aptitude"
for, so that whatever intelligence you were born with will at least
be put to the best use, and then work as hard as you can at it.
Whereas if intelligence isn't what matters, but only one of several
ingredients in what does, and many of those aren't inborn, things
get more interesting. You have a lot more control, but the problem
of how to arrange your life becomes that much more complicated.
 <br/>
 <br/>
 So what are the other ingredients in having new ideas? The fact
that I can even ask this question proves the point I raised earlier
— that society hasn't assimilated the fact that it's this and not
intelligence that matters. Otherwise we'd all know the answers
to such a fundamental question.
 <font color="#dddddd">
  [
  <a href="#f3n">
   <font color="#dddddd">
    3
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 I'm not going to try to provide a complete catalogue of the other
ingredients here. This is the first time I've posed
the question to myself this way, and I think it may take a while
to answer. But I wrote recently about one of the most important:
an obsessive
 <a href="genius.html">
  <u>
   interest
  </u>
 </a>
 in a particular topic. 
And this can definitely be cultivated.
 <br/>
 <br/>
 Another quality you need in order to discover new ideas is
 <a href="think.html">
  <u>
   independent-mindedness
  </u>
 </a>
 . I wouldn't want to 
claim that this is
distinct from intelligence — I'd be reluctant to call someone smart
who wasn't independent-minded — but though largely inborn, this
quality seems to be something that can be cultivated to some extent.
 <br/>
 <br/>
 There are general techniques for having new ideas — for example,
for working on your own
 <a href="own.html">
  <u>
   projects
  </u>
 </a>
 and
for overcoming the obstacles you face with
 <a href="early.html">
  <u>
   early
  </u>
 </a>
 work
— and these
can all be learned. Some of them can be learned by societies. And
there are also collections of techniques for generating specific types
of new ideas, like
 <a href="startupideas.html">
  startup ideas
 </a>
 and
 <a href="essay.html">
  essay topics
 </a>
 .
 <br/>
 <br/>
 And of course there are a lot of fairly mundane ingredients in
discovering new ideas, like
 <a href="hwh.html">
  <u>
   working hard
  </u>
 </a>
 , 
getting enough sleep, avoiding certain
kinds of stress, having the right colleagues, and finding tricks
for working on what you want even when it's not what you're supposed
to be working on. Anything that prevents people from doing great
work has an inverse that helps them to. And this class of ingredients
is not as boring as it might seem at first. For example, having new
ideas is generally associated with youth. But perhaps it's not youth
per se that yields new ideas, but specific things that come with
youth, like good health and lack of responsibilities. Investigating
this might lead to strategies that will help people of any age to
have better ideas.
 <br/>
 <br/>
 One of the most surprising ingredients in having new ideas is writing
ability. There's a class of new ideas that are best discovered by
writing essays and books. And that "by" is deliberate: you don't
think of the ideas first, and then merely write them down. There
is a kind of thinking that one does by writing, and if you're clumsy
at writing, or don't enjoy doing it, that will get in your way if
you try to do this kind of thinking.
 <font color="#dddddd">
  [
  <a href="#f4n">
   <font color="#dddddd">
    4
   </font>
  </a>
  ]
 </font>
 <br/>
 <br/>
 I predict the gap between intelligence and new ideas will turn out
to be an interesting place. If we think of this gap merely as a measure
of unrealized potential, it becomes a sort of wasteland we try to
hurry through with our eyes averted. But if we flip the question,
and start inquiring into the other ingredients in new ideas that
it implies must exist, we can mine this gap for discoveries about
discovery.
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 [
 ]
What wins in conversation depends on who with. It ranges from
mere aggressiveness at the bottom, through quick-wittedness in the
middle, to something closer to actual intelligence at the top,
though probably always with some component of quick-wittedness.
 [
 ]
Just as intelligence isn't the only ingredient in having new
ideas, having new ideas isn't the only thing intelligence is useful
for. It's also useful, for example, in diagnosing problems and figuring
out how to fix them. Both overlap with having new ideas, but both
have an end that doesn't.
 Those ways of using intelligence are much more common than having
new ideas. And in such cases intelligence is even harder to distinguish
from its consequences.
 [
 ]
Some would attribute the difference between intelligence and
having new ideas to "creativity," but this doesn't seem a very
useful term. As well as being pretty vague, it's shifted half a frame
sideways from what we care about: it's neither separable from
intelligence, nor responsible for all the difference between
intelligence and having new ideas.
 [
 ]
Curiously enough, this essay is an example. It started out
as an essay about writing ability. But when I came to the distinction
between intelligence and having new ideas, that seemed so much more
important that I turned the original essay inside out, making that
the topic and my original topic one of the points in it. As in many
other fields, that level of reworking is easier to contemplate once
you've had a lot of practice.
</font><br><br><b>Weird Languages</b><br><font face="verdana" size="2">
 August 2021
 <br/>
 <br/>
 When people say that in their experience all programming languages
are basically equivalent, they're making a statement not about
languages but about the kind of programming they've done.
 <br/>
 <br/>
 99.5% of programming consists of gluing together calls to library
functions. All popular languages are equally good at this. So one
can easily spend one's whole career operating in the intersection
of popular programming languages.
 <br/>
 <br/>
 But the other .5% of programming is disproportionately interesting.
If you want to learn what it consists of, the weirdness of weird
languages is a good clue to follow.
 <br/>
 <br/>
 Weird languages aren't weird by accident. Not the good ones, at
least. The weirdness of the good ones usually implies the existence
of some form of programming that's not just the usual gluing together
of library calls.
 <br/>
 <br/>
 A concrete example: Lisp macros. Lisp macros seem weird even to
many Lisp programmers. They're not only not in the intersection of
popular languages, but by their nature would be hard to implement
properly in a language without turning it into a dialect of
Lisp. And macros are definitely evidence of techniques that go
beyond glue programming. For example, solving problems by first
writing a language for problems of that type, and then writing
your specific application in it. Nor is this all you can do with
macros; it's just one region in a space of program-manipulating
techniques that even now is far from fully explored.
 <br/>
 <br/>
 So if you want to expand your concept of what programming can be,
one way to do it is by learning weird languages. Pick a language
that most programmers consider weird but whose median user is smart,
and then focus on the differences between this language and the
intersection of popular languages. What can you say in this language
that would be impossibly inconvenient to say in others? In the
process of learning how to say things you couldn't previously say,
you'll probably be learning how to think things you couldn't
previously think.
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <br/>
 <font color="888888">
  to Trevor Blackwell, Patrick Collison, Daniel Gackle, Amjad
Masad, and Robert Morris for reading drafts of this.
 </font>
</font><br><br>